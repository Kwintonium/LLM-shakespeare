# LLM-shakespeare
Created a mini-GPT using the complete Shakespeare dataset to produce responses replicating the training step of the vastly bigger (20000x) OpenAI Chat-GPT.  
Note that this GPT does not encode the data - just decodes it. The following results are nonsense, but the structure is there!  
Example Output:  
![alt text](https://github.com/Kwintonium/LLM-shakespeare/blob/main/shakespeare.png)
