# LLM-shakespeare
Created a mini-GPT using the complete Shakespeare dataset to produce responses replicating the training step of the vastly bigger (20000x) OpenAI Chat-GPT.
  
Note that this GPT does not encode the data - just decodes it. The following results are nonsense, but the structure is there!  
  
Example Output:  
![alt text](https://github.com/Kwintonium/LLM-shakespeare/blob/main/shakespeare.png)

[Link to my Google Colab](https://colab.research.google.com/drive/10U0uIJKVGw9wZhgdmHAp74boq1DjJja-)
